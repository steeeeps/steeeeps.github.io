<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>大数据 on steeeeps</title>
    <link>https://steeeeps.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/</link>
    <description>Recent content in 大数据 on steeeeps</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© steeeeps</copyright>
    <lastBuildDate>Fri, 12 Jan 2018 22:17:00 +0000</lastBuildDate><atom:link href="https://steeeeps.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>mongodb sink connector 使用方法</title>
      <link>https://steeeeps.github.io/posts/2018/kafka-mongodb-sink-connector-usage/</link>
      <pubDate>Fri, 12 Jan 2018 22:17:00 +0000</pubDate>
      
      <guid>https://steeeeps.github.io/posts/2018/kafka-mongodb-sink-connector-usage/</guid>
      <description>mongodb sink connector 大致用法与 jdbc sink connector 相似。只是 jdbc sink connector 是 Confluent 官方提供。
mongodb sink connector 是开源社区提供，我选择是的hpgrahsl/kafka-connect-mongodb。
mongodb sink connector 使用步骤   下载上述 kafka connector mongodb,执行 mvn clean package 对工程打包。
  将打包后的jar 包注册到java CLASSPATH 中，或者将打包后的kafka-connect-mongodb-1.0-SNAPSHOT-jar-with-dependencies.jar 文件拷贝至Confluent platform安装目录/share/java/confluent-common 中，我选择的是后者。
  在Confluent 安装目录/etc/ 目录下创建 mongodb 文件夹，将 kafka-connect-mongodb目录/config/MongoDbSinkConnector.properties 文件拷贝于此。关于此文件的配置信息请参考mongodb-sink-connector-configuration
  下载并安装 mongodb
  开启三个 terminal，分别执行
1) $ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties 2) $ ./bin/kafka-server-start ./etc/kafka/server.properties 3) $ ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties   以单机模式启动 connector</description>
    </item>
    
    <item>
      <title>JDBC sink connector 使用方法</title>
      <link>https://steeeeps.github.io/posts/2018/kafka-jdbc-sink-connector-usage/</link>
      <pubDate>Fri, 12 Jan 2018 22:03:00 +0000</pubDate>
      
      <guid>https://steeeeps.github.io/posts/2018/kafka-jdbc-sink-connector-usage/</guid>
      <description>Kafka Connect 是在0.9版本之后增加的新特性,可以更方便的创建和管理数据流管道。
它为Kafka和其它系统创建规模可扩展的、可信赖的流数据提供了一个简单的模型，通过connectors可以将大数据从其它系统导入到Kafka中，也可以从Kafka中导出到其它系统。
Kafka Connect可以将完整的数据库注入到Kafka的Topic中，或者将服务器的系统监控指标注入到Kafka，然后像正常的Kafka流处理机制一样进行数据流处理。而导出工作则是将数据从Kafka Topic中导出到其它数据存储系统、查询系统或者离线分析系统等，比如数据库、Elastic Search、Apache Ignite等。
下面以 Confluent platform 提供的 JDBC sink connector和社区提供的 mongodb sink connector 为例，介绍 connector 的用法。Confluent Platform 详细使用请看帮助文档
JDBC sink connector 使用步骤   下载 Confluent Platform 3.2,该版本使用的 Kafka 版本为 10.0.2.0。Confluent platform需要在 linux 或者 osx 下使用，同时需要/var/lib目录的写权限，window 暂时支持的不好。
  下载安装 sqlite
  进入 Confluent platform 的的文件目录
  在单独的 terminal 中启动 Zookeeper
$ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties   在单独的 terminal 中启动 kafka
$ ./bin/kafka-server-start .</description>
    </item>
    
    <item>
      <title>logstash 插件安装方式</title>
      <link>https://steeeeps.github.io/posts/2017/logstash-install-plugin/</link>
      <pubDate>Tue, 26 Dec 2017 22:10:49 +0000</pubDate>
      
      <guid>https://steeeeps.github.io/posts/2017/logstash-install-plugin/</guid>
      <description>logstash提供了很多filter、input、output插件，但是有些是需要自己安装的，以logstash-output-mongodb为例，纪录安装插件的方法。
在线安装 以下两个操作需要联网，并确保能访问https://rubygems.org/,如果不能访问可以使用代理或者修改logstash 安装目录下的 gemfile文件中的源为淘宝镜像https://ruby.taobao.org/。
直接安装   进入logstash安装目录，输入以下代码进行安装
bin/logstash-plugin install   源码安装   下载插件源码logstash-output-mongodb,并解压。
  打开解压目录，打开 gemfile文件并找到或者添加如下配置：
gem &amp;#34;logstash-output-mongodb&amp;#34;, :path =&amp;gt; &amp;#34;file:///C:/soft/logstash-output-mongodb-3.1.1&amp;#34;   执行代码
bin/logstash-plugin install --no-verify   安装成功
   这一步网上介绍的都是属于离线安装模式，但是在实际操作中还是 需要联网
 离线安装 生产环境很多服务器无法联网，无法安装插件，可以先在能够联网的服务器上使用以上两种方式进行安装，然后使用以下两种方式将插件拷贝至无法联网的服务器。
直接拷贝  拷贝 [logstash安装目录]/vender/bundle/jruby/1.9/gems/ 下面的你要的插件整个文件夹到目标机器同目录下。 拷贝 [logstash安装目录]/vendor/bundle/jruby/1.9/specifications/ 下面对应的声明文件到目标机器同目录下。 手动修改[logstash安装目录]/Gemfile 文件，加入插件行如：gem &amp;ldquo;logstash-output-mongodb&amp;rdquo;   这种方法我测试了，安装无效， 不推荐继续使用。
 离线打包 使用官方推荐的离线插件管理方法进行安装
  进入logstash安装目录，运行以下命令，对插件打包。
bin\logstash-plugin prepare-offline-pack --output logstash-output-mongodb.zip logstash-output-mongodb   将运行结果生成的logstash-output-mongodb.</description>
    </item>
    
    <item>
      <title>logstash 使用配置纪录</title>
      <link>https://steeeeps.github.io/posts/2017/logstash-useage/</link>
      <pubDate>Tue, 26 Dec 2017 22:02:27 +0000</pubDate>
      
      <guid>https://steeeeps.github.io/posts/2017/logstash-useage/</guid>
      <description>logstash是Elastic stack中最主要的数据获取、转换工作。主要记录一下日常使用中的配置信息。
导入带有地理位置的 csv文件到 ES 中 input { file { path =&amp;gt; &amp;#34;/home/steeeeps/workspace/es_datas/csv/sicuan.csv&amp;#34; # type =&amp;gt; &amp;#34;testeFile_lite&amp;#34; start_position =&amp;gt; &amp;#34;beginning&amp;#34; sincedb_path =&amp;gt; &amp;#34;/home/steeeeps/workspace/es_datas/logs&amp;#34; } } filter { csv{ columns =&amp;gt; [&amp;#39;poiid&amp;#39;, &amp;#39;title&amp;#39;, &amp;#39;address&amp;#39;,&amp;#39;lon&amp;#39;,&amp;#39;lat&amp;#39;,&amp;#39;city&amp;#39;,&amp;#39;category_name&amp;#39;,&amp;#39;checkun_num&amp;#39;,&amp;#39;photo_num&amp;#39;] separator =&amp;gt; &amp;#34;,&amp;#34; } mutate { add_field =&amp;gt; { &amp;#34;[location][lat]&amp;#34; =&amp;gt; &amp;#34;%{lat}&amp;#34; } add_field =&amp;gt; { &amp;#34;[location][lon]&amp;#34; =&amp;gt; &amp;#34;%{lon}&amp;#34; } } mutate { convert =&amp;gt; { &amp;#34;[location][lat]&amp;#34; =&amp;gt; &amp;#34;float&amp;#34; &amp;#34;[location][lon]&amp;#34; =&amp;gt; &amp;#34;float&amp;#34; } } } output { elasticsearch { # action =&amp;gt; &amp;#34;index&amp;#34; hosts =&amp;gt; [&amp;#34;http://172.</description>
    </item>
    
  </channel>
</rss>
